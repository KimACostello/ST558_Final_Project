---
title: "Modeling"
author: "Kim Costello"
format: html
editor: visual
---
## Models to Predict Diabetes Status

```{r}
library(tidyverse)
library(tidymodels)
library(rpart)
library(ranger)
```

Before we fit our models, we will want to split the data into training and test sets, and create our cross-validation folds since we will be comparing different models.  
```{r}
set.seed(123)  #set seed to make things reproducible
diabetes_split <- initial_split(diabetes_data, prop = .70)
diabetes_train <- training(diabetes_split)
diabetes_test <- testing(diabetes_split)
diabetes_5fold <- vfold_cv(diabetes_train, 5)
```

#### Classification Tree Model

Tree based models split up predictor space into regions. On each region, a different prediction can then be made, and adjacent regions do not need to have predictions close to each other. A classification tree is used when we have a categorical response variable. The goal of a classification tree is to classify (predict) group membership, which is usually the most prevalent class in region as the prediction. 

Let's create a recipe that can be used for both the classification tree and the random forest model. 
```{r}
diabetes_recipe <- recipe(diabetes_binary ~ ., data = diabetes_train) |>
  update_role(age, new_role = "ID") |>
  step_rm(phys_hlth, ment_hlth, any_healthcare) |>
  step_dummy(high_bp, high_chol, smoker, stroke, phys_activity,
             heart_diseaseor_attack, hvy_alcohol_consump, sex, gen_hlth) |>
  step_normalize(bmi)

diabetes_recipe
```

Define the model and engine for the classification tree. 
```{r}
tree_model <- decision_tree(tree_depth = tune(),
                            min_n = 20,
                            cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Create workflow by adding the recipe and the model.
```{r}
tree_workflow <- workflow() |>
  add_recipe(diabetes_recipe) |>
  add_model(tree_model)
```

Set up grid of tuning parameters. 
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10, 5))
```

Fit workflow to CV folds. 
```{r}
class_wkf_folds <- tree_workflow |>
  tune_grid(resamples = diabetes_5fold,
            metrics = metric_set(mn_log_loss),
            grid = tree_grid)

class_wkf_folds |>
  collect_metrics() |>
  arrange(mean)
```

Grab the best model's tuning parameter values. 
```{r}
best_class_tree <- select_best(class_wkf_folds, metric = "mn_log_loss")
best_class_tree
```

Finalize workflow and fit to the entire training data. 
```{r}
tree_final_wkf <- tree_workflow |>
  finalize_workflow(best_class_tree)
```

```{r}
final_tree_fit <- tree_final_wkf |>
  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))

```

#### Random Forest

A random forest model is an ensemble tree method. It uses bootstrapping to get multiple samples to fit on, and then averages across the many fitted trees for final prediction. It decreases variance over an individual tree fit. The big difference is that it does not use all predictors at each step (at each split of each tree). It will use a random subset of predictors each time. The trees can be very different from each other, which can lead to better predictions overall. 

We will use the same recipe, `tree_recipe`, as before. 

Define model, engine, and mode for the random forest model (with mtry as the tuning parameter). 
```{r}
forest_model <- rand_forest(mtry = tune(), trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")
```

Create workflow for the random forest model.
```{r}
forest_workflow <- workflow() |>
  add_recipe(diabetes_recipe) |>
  add_model(forest_model)
```

Fit to CV folds. 
```{r}
forest_fit <- forest_workflow |>
  tune_grid(resamples = diabetes_5fold,
            grid = 5,
            metrics = metric_set(mn_log_loss))
```

Check metrics across folds and look at log loss. 
```{r}
forest_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

Grab best tuning parameter. 
```{r}
forest_best_param <- select_best(forest_fit, metric = "mn_log_loss")
forest_best_param
```
4 randomly selected predictors was best when we used cross validation.

Refit on entire training set using this tuning parameter. 
```{r}
forest_final_wf <- forest_workflow |>
  finalize_workflow(forest_best_param)

forest_final_fit <- forest_final_wf |>
  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))
```


#### Compare Models

Now let's compare models by looking at our metric, log loss, for each model. 
```{r}
rbind(
  final_tree_fit |>
    collect_metrics() |>
    mutate(Model = "Tree", .before = ".metric"),
  forest_final_fit |>
    collect_metrics() |>
    mutate(Model = "Forest", .before = ".metric")
)
```

We see the best model (by log loss) is the Random Forest Model.
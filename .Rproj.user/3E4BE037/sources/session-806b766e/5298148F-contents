---
title: "Homework 8: Basic Modeling Practice"
author: "Kim Costello"
format: html
editor: visual
---

### Seoul Bike Sharing Demand Dataset

```{r}
library(readr)
library(broom)
library(lubridate)
library(janitor)
library(dplyr)
library(ggplot2)
library(GGally)
library(tidymodels)
```

The following data set is provided by the UCI Machine Learning Repository. The data set contains count of public bicycles rented per hours in the Seoul Bike Sharing System, with corresponding weather data and holiday information.

The following variables are provided:

-   Date - day/month/year

-   Rented Bike count - Count of bikes rented at each hour

-   Hour - Hour of the day

-   Temperature-Temperature in Celsius

-   Humidity - %

-   Windspeed - m/s

-   Visibility - 10m

-   Dew point temperature - Celsius

-   Solar radiation - MJ/m2

-   Rainfall - mm

-   Snowfall - cm

-   Seasons - Winter, Spring, Summer, Autumn

-   Holiday - Holiday/No holiday

-   Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)

```{r}
# Read in data
bike_data <- read_csv("SeoulBikeData.csv", locale = locale(encoding = "latin1"))
```

#### EDA

Before we summarize the data, let's determine if the data set contains any missing values.

```{r}
# check for missing values
colSums(is.na(bike_data))
```

The data provided does not contain any missing values.

The following contains the structure of the data set, so we can see the different variables and how they are being stored.

```{r}
str(bike_data)
```

Below is a summary of basic stats that can be used to make sure the data makes sense.

```{r}
summary(bike_data)
```

```{r}
unique(bike_data$Seasons)
unique(bike_data$Holiday)
unique(bike_data$`Functioning Day`)
```

Unique values for Seasons are Winter, Spring, Summer, and Autumn. The holiday variable value for an observation is either No Holiday or Holiday. The Functioning Day is either Yes or No (Yes, it is during functional hours or No, it is not).

Let's manipulate the data by:

-   converting the date column to an actual date.

-   turning the character variables into factors (for the different levels/categories).

-   renaming all the variables to snake case.

```{r}
bike_data <- bike_data |>
  mutate(
     Date = dmy(Date),
     Seasons = as.factor(Seasons),
     Holiday = as.factor(Holiday),
     `Functioning Day` = as.factor(`Functioning Day`)
  ) |>
  clean_names("snake")
  
```

Let's see how many observations occurred in each season during functional or non-functional hours. And let's see how many observations occurred during a holiday vs no holiday during functional and non-functional hours.

```{r}
# Two-way contingency tables
table(bike_data$functioning_day, bike_data$seasons)
table(bike_data$functioning_day, bike_data$holiday)
```

There are no observations in Summer and Winter that occurred during non-functional hours. Most observations are not on a holiday and during functional hours.

Let's look at the Mean and Median for all numeric variables during functional and non-functional hours.

```{r}
bike_data |>
  group_by(functioning_day) |>
  summarize(across(where(is.numeric), 
                   list("mean" = mean, "median" = median), 
                   .names = "{.fn}_{.col}"))
```

The mean and median rented bike counts are zero during non-functional hours.

To confirm there are no bikes being rented during non-functional hours, let's look at the min and max rental bike counts.

```{r}
bike_data |>
  group_by(functioning_day) |>
  summarize(min_bike_count = min(rented_bike_count), max_bike_count = max(rented_bike_count))
  
```

No bikes were rented during non-functional hours.

Since no bikes are being rented during non-functional hours, we can remove the observations that occurred during non-functional hours.

```{r}
fun_bike_data <- bike_data |>
  filter(functioning_day == "Yes")
```

To simplify analysis, let's summarize across the hours so each day has one observation with it. This can be done by grouping by character variables, and summarizing (either sum or mean) across numeric variables.

```{r}
bike_summary_data <- fun_bike_data |>
  group_by(date, seasons, holiday) |>
  summarize(
    sum_bike_count = sum(rented_bike_count),
    sum_rainfall = sum(rainfall_mm),
    sum_snowfall = sum(snowfall_cm),
    across(c(temperature_c, 
             humidity_percent,
             wind_speed_m_s,
             visibility_10m,
             dew_point_temperature_c,
             solar_radiation_mj_m2), 
           ~ mean(.x),
           .names = "mean_{.col}")
    
  )

head(bike_summary_data)

    
```

Now that non-functional hours have been removed from the data, let's see the number of observations that occurred on a holiday or no holiday by the season.

```{r}
table(bike_summary_data$seasons, bike_summary_data$holiday)
```

Let's get the mean and median of numeric variables without the non-functional hours observations, by seasons.

```{r}
bike_summary_data |>
  group_by(seasons) |>
  summarize(across(where(is.numeric), 
                   list("mean" = mean, "median" = median), 
                   .names = "{.fn}_{.col}"))
```

Not surprisingly, the most bike rentals occurred during the Summer, then Autumn.

Let's see the min and max daily bike rentals by season.

```{r}
bike_summary_data |>
  group_by(seasons) |>
  summarize(min_bike_count = min(sum_bike_count), max_bike_count = max(sum_bike_count))
```

Let's generate a correlation matrix plot, so we can see the relationships between the numeric variables.

```{r}
df_corr <- bike_summary_data |>
  select(where(is.numeric)) |>
  rename("Bike_count" = sum_bike_count,
         "Rainfall" = sum_rainfall,
         "Snowfall" = sum_snowfall,
         "Temperature" = mean_temperature_c,
         "Humidity" = mean_humidity_percent,
         "Windspeed" = mean_wind_speed_m_s,
         "Visibility" = mean_visibility_10m,
         "Dew_point" = mean_dew_point_temperature_c,
         "Solar_radiation" = mean_solar_radiation_mj_m2)

ggcorr(df_corr, 
       nbreaks = 6,
       label = TRUE,
       palette = "BuPu",
       size = 2,          # adjusting variable names sizes
       hjust = 0.65) +    # adjusting position of variable names) 
  labs(title = "Correlation Matrix of all Numeric Variables") +
  ggeasy::easy_center_title()
```

When looking at daily bike rentals, temperature has the greatest positive correlation (the higher the temp, the higher the \# of bike rentals). Humidity does not appear to have any relationship with bike rental counts.

```{r}
df_corr |>
  group_by(seasons) |>
  summarize(correlation = cor(Bike_count, Temperature, use = "pairwise.complete.obs"))
```

When looking at the relationship between bike counts and temperature across seasons, it's surprising to see that summer has a slight negative correlation. Spring has a greater positive correlation between temperature and bike counts.

Let's see some plots to explore the data.

Scatterplot:

```{r}
g <- ggplot(data = bike_summary_data)

g + geom_point(aes(x = sum_bike_count, 
                   y = sum_rainfall, 
                   color = seasons)) +
  labs(x = "Rental Bike Count", 
       y = "Rainfall (mm)",
       title = "Rental Bike Count by Total Daily Rainfall",
       color = "Seasons") +
  scale_color_manual(values = c("#FFFF00",
                                "#FF00FF",
                                "#7FFF00",
                                "#00FFFF")) +
  theme_dark() +
  ggeasy::easy_center_title() 
```

In the plot above, you can see rental bike counts vs rainfall. The points are also color coded by season. Bike rentals are greater in the summer, but there are fewer observations during heavy rainfall.

Boxplot:

```{r}
g + geom_boxplot(aes(x = seasons, y = sum_bike_count, fill = holiday)) +
  labs(x = "Seasons", 
       y = "Daily Rental Bike Count",
       title = "Daily Rental Bike Count by Seasons",
       fill = "Holiday") +
  ggeasy::easy_center_title() +
  scale_fill_manual(values = c("lightpink",
                               "lightblue"))
```

In the plot above, you can see that bike rental counts are greater in Summer and Autumn. Spring has a larger range of bike counts on a given day (greater distance between the min and max).

Scatterplot with a regression line:

```{r}
ggplot(bike_summary_data, aes(x = date, y = sum_bike_count)) +
  geom_point() +
  labs(x = "Date", 
       y = "Daily Rental Bike Count",
       title = "Daily Rental Bike Count by Date") +
  ggeasy::easy_center_title() +
  geom_smooth()
```

In the plot above, you can see the trend of daily bike rentals by date, throughout 2018. Between June and July appear to have the most daily bike rentals, and January has the lowest.

#### Splitting the Data

Split the data into training and test set.

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(123)
# Put 3/4 of the data into the training set 
data_split <- initial_split(bike_summary_data, prop = 3/4, strata = seasons)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

On the training set, create a 10 fold cross validation split.

```{r}
# Create cross validation folds on the training data
set.seed(234)
bike_folds <- vfold_cv(train_data, 10)
```

#### Fitting MLR Models

Let's create some recipes.

```{r}
recipe_one <- recipe(sum_bike_count ~ ., data = train_data) |> 
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow") |>
  step_mutate(day_of_week = 
                as.factor(ifelse(date_dow %in% c("Sat", "Sun"), "weekend", "weekday"))) |>
  step_rm(date_dow) |>     #removes intermediate variable
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_of_week)

```

```{r}
# Same as recipe one except including interactions.
recipe_two <- recipe(sum_bike_count ~ ., data = train_data) |> 
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow") |>
  step_mutate(day_of_week = 
                as.factor(ifelse(date_dow %in% c("Sat", "Sun"), "weekend", "weekday"))) |>
  step_rm(date_dow) |>     #removes intermediate variable
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_of_week) |>
  step_interact(terms = ~starts_with("holiday")*starts_with("seasons") +
                  ~mean_temperature_c*starts_with("seasons") +
                  ~mean_temperature_c*sum_rainfall)
  

```

```{r}
# Same as recipe two except adding quadratic terms to numeric predictors.
recipe_three <- recipe(sum_bike_count ~ ., data = train_data) |> 
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow") |>
  step_mutate(day_of_week = 
                as.factor(ifelse(date_dow %in% c("Sat", "Sun"), "weekend", "weekday"))) |>
  step_rm(date_dow) |>     #removes intermediate variable
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_of_week) |>
  step_interact(terms = ~starts_with("holiday")*starts_with("seasons") +
                  ~mean_temperature_c*starts_with("seasons") +
                  ~mean_temperature_c*sum_rainfall) |>
  step_poly(starts_with("mean"), sum_rainfall, sum_snowfall)
```

Now, let's set up our linear model fit to use the lm engine.

```{r}
bike_mod <- linear_reg() |>
  set_engine("lm")
```

Combining recipes and model into workflows (for each recipe).

```{r}
workflow_one <- workflow() |>
  add_recipe(recipe_one) |>
  add_model(bike_mod)

workflow_two <- workflow() |>
  add_recipe(recipe_two) |>
  add_model(bike_mod)

workflow_three <- workflow() |>
  add_recipe(recipe_three) |>
  add_model(bike_mod)

```

Fit the models using the 10 fold CV.

```{r}
bike_CV_fit_one <- workflow_one |>
  fit_resamples(bike_folds)
bike_CV_fit_one

bike_CV_fit_one |>
  collect_metrics()
```

```{r}
bike_CV_fit_two <- workflow_two |>
  fit_resamples(bike_folds)
bike_CV_fit_two

bike_CV_fit_two |>
  collect_metrics()
```

```{r}
bike_CV_fit_three <- workflow_three |>
  fit_resamples(bike_folds)
bike_CV_fit_three

bike_CV_fit_three |>
  collect_metrics()
```

Model 3 is the best fit (lowest RMSE and highest RSQ).

Using the "best" model, fit th emodel to the entire training data set.

```{r}
set.seed(123)
best_fit <- 
  workflow_three |>
  last_fit(data_split)

best_fit |>
  collect_metrics()
```

Make predictions on the test set and calculate RMSE.

```{r}
test_fit <- extract_workflow(best_fit)
bike_predictions <- predict(test_fit, new_data = test_data) |>
  bind_cols(test_data |>
              select(sum_bike_count))   #add the true values

RMSE_test <- bike_predictions |>
  rmse(truth = sum_bike_count, estimate = .pred)
RMSE_test
```

Obtain the final model coefficient table.

```{r}
best_fit |>
  extract_fit_parsnip() |>
  tidy()
```

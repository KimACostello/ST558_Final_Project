[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(rpart)\n\nWarning: package 'rpart' was built under R version 4.5.2\n\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.5.2\n\n\n\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBefore we fit our models, we will want to split the data into training and test sets, and create our cross-validation folds since we will be comparing different models.\n\nset.seed(123)  #set seed to make things reproducible\ndiabetes_split &lt;- initial_split(diabetes_data, prop = .70)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_5fold &lt;- vfold_cv(diabetes_train, 5)\n\n\n\nTree based models split up predictor space into regions. On each region, a different prediction can then be made, and adjacent regions do not need to have predictions close to each other. A classification tree is used when we have a categorical response variable. The goal of a classification tree is to classify (predict) group membership, which is usually the most prevalent class in region as the prediction.\nLet’s create a recipe that can be used for both the classification tree and the random forest model.\n\ndiabetes_recipe &lt;- recipe(diabetes_binary ~ ., data = diabetes_train) |&gt;\n  update_role(age, new_role = \"ID\") |&gt;\n  step_rm(phys_hlth, ment_hlth, any_healthcare) |&gt;\n  step_dummy(high_bp, high_chol, smoker, stroke, phys_activity,\n             heart_diseaseor_attack, hvy_alcohol_consump, sex, gen_hlth) |&gt;\n  step_normalize(bmi)\n\ndiabetes_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\nID:         1\n\n\n\n\n\n── Operations \n\n\n• Variables removed: phys_hlth, ment_hlth, any_healthcare\n\n\n• Dummy variables from: high_bp, high_chol, smoker, stroke, ...\n\n\n• Centering and scaling for: bmi\n\n\nDefine the model and engine for the classification tree.\n\ntree_model &lt;- decision_tree(tree_depth = tune(),\n                            min_n = 20,\n                            cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nCreate workflow by adding the recipe and the model.\n\ntree_workflow &lt;- workflow() |&gt;\n  add_recipe(diabetes_recipe) |&gt;\n  add_model(tree_model)\n\nSet up grid of tuning parameters.\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\nFit workflow to CV folds.\n\nclass_wkf_folds &lt;- tree_workflow |&gt;\n  tune_grid(resamples = diabetes_5fold,\n            metrics = metric_set(mn_log_loss),\n            grid = tree_grid)\n\nclass_wkf_folds |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001         11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 2    0.000000001          11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 3    0.00000001           11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 4    0.0000001            11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 5    0.000001             11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 6    0.00001              11 mn_log_loss binary     0.335     5 0.00263 pre0_m…\n 7    0.0000000001          8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n 8    0.000000001           8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n 9    0.00000001            8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n10    0.0000001             8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n# ℹ 40 more rows\n\n\nGrab the best model’s tuning parameter values.\n\nbest_class_tree &lt;- select_best(class_wkf_folds, metric = \"mn_log_loss\")\nbest_class_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1    0.0000000001         11 pre0_mod04_post0\n\n\nFinalize workflow and fit to the entire training data.\n\ntree_final_wkf &lt;- tree_workflow |&gt;\n  finalize_workflow(best_class_tree)\n\n\nfinal_tree_fit &lt;- tree_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n\n\n\nA random forest model is an ensemble tree method. It uses bootstrapping to get multiple samples to fit on, and then averages across the many fitted trees for final prediction. It decreases variance over an individual tree fit. The big difference is that it does not use all predictors at each step (at each split of each tree). It will use a random subset of predictors each time. The trees can be very different from each other, which can lead to better predictions overall.\nWe will use the same recipe, tree_recipe, as before.\nDefine model, engine, and mode for the random forest model (with mtry as the tuning parameter).\n\nforest_model &lt;- rand_forest(mtry = tune(), trees = 100) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\nCreate workflow for the random forest model.\n\nforest_workflow &lt;- workflow() |&gt;\n  add_recipe(diabetes_recipe) |&gt;\n  add_model(forest_model)\n\nFit to CV folds.\n\nforest_fit &lt;- forest_workflow |&gt;\n  tune_grid(resamples = diabetes_5fold,\n            grid = 5,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n\nCheck metrics across folds and look at log loss.\n\nforest_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1     4 mn_log_loss binary     0.323     5 0.00112  pre0_mod2_post0\n2     7 mn_log_loss binary     0.331     5 0.00133  pre0_mod3_post0\n3    10 mn_log_loss binary     0.345     5 0.00177  pre0_mod4_post0\n4     1 mn_log_loss binary     0.348     5 0.000869 pre0_mod1_post0\n5    13 mn_log_loss binary     0.435     5 0.00546  pre0_mod5_post0\n\n\nGrab best tuning parameter.\n\nforest_best_param &lt;- select_best(forest_fit, metric = \"mn_log_loss\")\nforest_best_param\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;int&gt; &lt;chr&gt;          \n1     4 pre0_mod2_post0\n\n\n4 randomly selected predictors was best when we used cross validation.\nRefit on entire training set using this tuning parameter.\n\nforest_final_wf &lt;- forest_workflow |&gt;\n  finalize_workflow(forest_best_param)\n\nforest_final_fit &lt;- forest_final_wf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n\n\n\nNow let’s compare models by looking at our metric, log loss, for each model.\n\nrbind(\n  final_tree_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"Tree\", .before = \".metric\"),\n  forest_final_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"Forest\", .before = \".metric\")\n)\n\n# A tibble: 2 × 5\n  Model  .metric     .estimator .estimate .config        \n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Tree   mn_log_loss binary         0.336 pre0_mod0_post0\n2 Forest mn_log_loss binary         0.325 pre0_mod0_post0\n\n\nWe see the best model (by log loss) is the Random Forest Model."
  },
  {
    "objectID": "Modeling.html#models-to-predict-diabetes-status",
    "href": "Modeling.html#models-to-predict-diabetes-status",
    "title": "Modeling",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.5.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n\n\nWarning: package 'dials' was built under R version 4.5.2\n\n\nWarning: package 'infer' was built under R version 4.5.2\n\n\nWarning: package 'modeldata' was built under R version 4.5.2\n\n\nWarning: package 'parsnip' was built under R version 4.5.2\n\n\nWarning: package 'recipes' was built under R version 4.5.2\n\n\nWarning: package 'rsample' was built under R version 4.5.2\n\n\nWarning: package 'tailor' was built under R version 4.5.2\n\n\nWarning: package 'tune' was built under R version 4.5.2\n\n\nWarning: package 'workflows' was built under R version 4.5.2\n\n\nWarning: package 'workflowsets' was built under R version 4.5.2\n\n\nWarning: package 'yardstick' was built under R version 4.5.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(rpart)\n\nWarning: package 'rpart' was built under R version 4.5.2\n\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.5.2\n\n\n\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nBefore we fit our models, we will want to split the data into training and test sets, and create our cross-validation folds since we will be comparing different models.\n\nset.seed(123)  #set seed to make things reproducible\ndiabetes_split &lt;- initial_split(diabetes_data, prop = .70)\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\ndiabetes_5fold &lt;- vfold_cv(diabetes_train, 5)\n\n\n\nTree based models split up predictor space into regions. On each region, a different prediction can then be made, and adjacent regions do not need to have predictions close to each other. A classification tree is used when we have a categorical response variable. The goal of a classification tree is to classify (predict) group membership, which is usually the most prevalent class in region as the prediction.\nLet’s create a recipe that can be used for both the classification tree and the random forest model.\n\ndiabetes_recipe &lt;- recipe(diabetes_binary ~ ., data = diabetes_train) |&gt;\n  update_role(age, new_role = \"ID\") |&gt;\n  step_rm(phys_hlth, ment_hlth, any_healthcare) |&gt;\n  step_dummy(high_bp, high_chol, smoker, stroke, phys_activity,\n             heart_diseaseor_attack, hvy_alcohol_consump, sex, gen_hlth) |&gt;\n  step_normalize(bmi)\n\ndiabetes_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\nID:         1\n\n\n\n\n\n── Operations \n\n\n• Variables removed: phys_hlth, ment_hlth, any_healthcare\n\n\n• Dummy variables from: high_bp, high_chol, smoker, stroke, ...\n\n\n• Centering and scaling for: bmi\n\n\nDefine the model and engine for the classification tree.\n\ntree_model &lt;- decision_tree(tree_depth = tune(),\n                            min_n = 20,\n                            cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nCreate workflow by adding the recipe and the model.\n\ntree_workflow &lt;- workflow() |&gt;\n  add_recipe(diabetes_recipe) |&gt;\n  add_model(tree_model)\n\nSet up grid of tuning parameters.\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n\nFit workflow to CV folds.\n\nclass_wkf_folds &lt;- tree_workflow |&gt;\n  tune_grid(resamples = diabetes_5fold,\n            metrics = metric_set(mn_log_loss),\n            grid = tree_grid)\n\nclass_wkf_folds |&gt;\n  collect_metrics() |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001         11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 2    0.000000001          11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 3    0.00000001           11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 4    0.0000001            11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 5    0.000001             11 mn_log_loss binary     0.334     5 0.00236 pre0_m…\n 6    0.00001              11 mn_log_loss binary     0.335     5 0.00263 pre0_m…\n 7    0.0000000001          8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n 8    0.000000001           8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n 9    0.00000001            8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n10    0.0000001             8 mn_log_loss binary     0.336     5 0.00136 pre0_m…\n# ℹ 40 more rows\n\n\nGrab the best model’s tuning parameter values.\n\nbest_class_tree &lt;- select_best(class_wkf_folds, metric = \"mn_log_loss\")\nbest_class_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1    0.0000000001         11 pre0_mod04_post0\n\n\nFinalize workflow and fit to the entire training data.\n\ntree_final_wkf &lt;- tree_workflow |&gt;\n  finalize_workflow(best_class_tree)\n\n\nfinal_tree_fit &lt;- tree_final_wkf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n\n\n\nA random forest model is an ensemble tree method. It uses bootstrapping to get multiple samples to fit on, and then averages across the many fitted trees for final prediction. It decreases variance over an individual tree fit. The big difference is that it does not use all predictors at each step (at each split of each tree). It will use a random subset of predictors each time. The trees can be very different from each other, which can lead to better predictions overall.\nWe will use the same recipe, tree_recipe, as before.\nDefine model, engine, and mode for the random forest model (with mtry as the tuning parameter).\n\nforest_model &lt;- rand_forest(mtry = tune(), trees = 100) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\nCreate workflow for the random forest model.\n\nforest_workflow &lt;- workflow() |&gt;\n  add_recipe(diabetes_recipe) |&gt;\n  add_model(forest_model)\n\nFit to CV folds.\n\nforest_fit &lt;- forest_workflow |&gt;\n  tune_grid(resamples = diabetes_5fold,\n            grid = 5,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\n\nCheck metrics across folds and look at log loss.\n\nforest_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config        \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1     4 mn_log_loss binary     0.323     5 0.00112  pre0_mod2_post0\n2     7 mn_log_loss binary     0.331     5 0.00133  pre0_mod3_post0\n3    10 mn_log_loss binary     0.345     5 0.00177  pre0_mod4_post0\n4     1 mn_log_loss binary     0.348     5 0.000869 pre0_mod1_post0\n5    13 mn_log_loss binary     0.435     5 0.00546  pre0_mod5_post0\n\n\nGrab best tuning parameter.\n\nforest_best_param &lt;- select_best(forest_fit, metric = \"mn_log_loss\")\nforest_best_param\n\n# A tibble: 1 × 2\n   mtry .config        \n  &lt;int&gt; &lt;chr&gt;          \n1     4 pre0_mod2_post0\n\n\n4 randomly selected predictors was best when we used cross validation.\nRefit on entire training set using this tuning parameter.\n\nforest_final_wf &lt;- forest_workflow |&gt;\n  finalize_workflow(forest_best_param)\n\nforest_final_fit &lt;- forest_final_wf |&gt;\n  last_fit(diabetes_split, metrics = metric_set(mn_log_loss))\n\n\n\n\nNow let’s compare models by looking at our metric, log loss, for each model.\n\nrbind(\n  final_tree_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"Tree\", .before = \".metric\"),\n  forest_final_fit |&gt;\n    collect_metrics() |&gt;\n    mutate(Model = \"Forest\", .before = \".metric\")\n)\n\n# A tibble: 2 × 5\n  Model  .metric     .estimator .estimate .config        \n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 Tree   mn_log_loss binary         0.336 pre0_mod0_post0\n2 Forest mn_log_loss binary         0.325 pre0_mod0_post0\n\n\nWe see the best model (by log loss) is the Random Forest Model."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(readr)    #to read in CSV file\nlibrary(psych)    #to describe the data\nlibrary(dplyr)    #to clean the data\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(janitor)  #to clean names\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(ggplot2)  #for plots\n\n\nAttaching package: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\nlibrary(ggeasy)   #for plots"
  },
  {
    "objectID": "EDA.html#diabetes-health-indicators-dataset",
    "href": "EDA.html#diabetes-health-indicators-dataset",
    "title": "EDA",
    "section": "Diabetes Health Indicators Dataset",
    "text": "Diabetes Health Indicators Dataset\n\nIntro:\nThe Diabetes Health Indicators Dataset is a collection of information from telephone surveys that are conducted annually by the CDC. The survey collects responses from over 400,000 Americans on health-related risk behaviors. The data provided and explored within this document is from the year 2015.\nNumerous variables are provided in the dataset. However, we will focus on the variables listed below to explore, in relation to our response variable, Diabetes Status. This response variable is provided as a binary variable (diabetes_binary). A 0 value indicates the individual does not have diabetes and a 1 value indicates the individual has diabetes or pre-diabetes.\nOther variables of interest, and their possible values (where applicable):\n\nHighBP: 0 indicates no high blood pressure, 1 indicates high blood pressure\nHighChol: 0 indicates no high cholesterol, 1 indicates high cholesterol\nBMI: Body Mass Index, this is a measure of weight relative to height\nSmoker: 0 indicates individual has smoked at least 100 cigarettes in their life, 1 indicates they have not\nStroke: 0 indicates individual has not had a stroke, 1 indicates they have\nHeartDiseaseorAttack: 0 indicates individual has not had coronary heart disease or myocardial infarction, 1 indicates they have\nPhysActivity: 0 indicates individual has not had any physical activity (outside of job) in the past 30 days, 1 indicates they have\nHvyAlcoholConsump: 0 indicates individual is not a heavy drinker, 1 indicates they are (greater than or equal to 14 drinks for men and 7 drinks for women per week)\nAnyHealthcare: 0 indicates individual does not have health care coverage, 1 indicates they do\nGenHlth: general health level, 1 is excellent, 2 is very good, 3 is good, 4 is fair, and 5 is poor\nMentHlth: number of days of poor mental health (1-30 days)\nPhysHlth: number of days of physical illness or injury in the past 30 days\nSex: Female or Male\nAge: Age level category, 1 = 18-24, 2 = 25-29, 3 = 30-34, 4 = 35-39, 5 = 40-44, 6 = 45-49, 7 = 50-54, 8 = 55-59, 9 = 60-64, 10 = 65-69, 11 = 70-74, 12 = 75-79, 13 =80 or older\n\nFirst, exploratory data analysis will be conducted to get to know the data. We want to understand how the data is stored, determine the rate of missing values, and clean up the data as needed. We want to investigate the distributions with summaries and graphs to gain some insights into the data and relationships between variables.\nThen, we will want to determine which predictive model, a classification tree or a random forest model, is the best model to use to make a prediction for the response variable, Diabetes Status, given specific predictor variables.\n\n\nThe Data:\nThe data can be found on kaggle.com and is provided in a CSV file.\n\n#read in data\ndiabetes_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s determine if there are any missing values in the dataset.\n\ncolSums(is.na(diabetes_data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThere are no missing values in the dataset.\nThe following contains the structure of the data set, so we can see the different variables and how they are being stored.\n\nstr(diabetes_data)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nAll of the variables are being stored as numeric. However, most of these variables should be categorical - they have different levels or categories, and you cannot do math on the values (the results would not make sense). Therefore, we will want to convert them to factors.\n\ndiabetes_data &lt;- diabetes_data |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, \n                             levels = c(0, 1),\n                             labels = c(\"No diabetes\",\n                                        \"Prediabetes/Diabetes\")),\n    HighBP = factor(HighBP,\n                    levels = c(0, 1),\n                    labels = c(\"Not high BP\", \"High BP\")),\n    HighChol = factor(HighChol,\n                      levels = c(0, 1),\n                      labels = c(\"Not high cholesterol\", \"High cholesterol\")), \n    Smoker = factor(Smoker,\n                      levels = c(0, 1),\n                      labels = c(\"Non-smoker\", \"Smoker\")),\n    Stroke = factor(Stroke,\n                      levels = c(0, 1),\n                      labels = c(\"No stroke\", \"Stroke\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack,\n                                  levels = c(0, 1),\n                                  labels = c(\"No heart disease/attack\", \n                                             \"Heart disease/attack\")),\n    PhysActivity = factor(PhysActivity,\n                      levels = c(0, 1),\n                      labels = c(\"No physical activity\", \"Physical activity\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump,\n                      levels = c(0, 1),\n                      labels = c(\"Not a heavy drinker\", \"Heavy drinker\")),\n    AnyHealthcare = factor(AnyHealthcare,\n                      levels = c(0, 1),\n                      labels = c(\"No healthcare\", \"Healthcare\")),\n    GenHlth = factor(GenHlth,\n                     levels = c(1, 2, 3, 4, 5),\n                     labels = c(\"Excellent\", \"Very good\", \n                                \"Good\", \"Fair\", \"Poor\")),\n    Sex = factor(Sex,\n                 levels = c(0, 1),\n                 labels = c(\"Female\", \"Male\")),\n    Age = factor(Age,\n                 levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13),\n                 labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\",\n                            \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\",\n                            \"70-74\", \"75-79\", \"80 or older\"))\n    ) |&gt;\n  select(-CholCheck, -NoDocbcCost, -Fruits, -Veggies, -Income, -Education, \n         -DiffWalk) |&gt; #removing variables that we won't use\n  clean_names(\"snake\")     #cleaning names for easier coding\n\n\n\nData Summaries and Plots\nLet’s explore the data set to gain some insights into the distributions and relationships.\nThe following is the mean, median, minimum, maximum, and interquartile range for the numeric variables, grouped by diabetes status.\n\ndiabetes_data |&gt;\n  group_by(diabetes_binary) |&gt;\n  summarize(across(where(is.numeric), \n                   list(\"mean\" = mean, \n                        \"median\" = median, \n                        \"min\" = min, \n                        \"max\" = max,\n                        \"IQR\" = IQR), \n                   .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 16\n  diabetes_binary     mean_bmi median_bmi min_bmi max_bmi IQR_bmi mean_ment_hlth\n  &lt;fct&gt;                  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 No diabetes             27.8         27      12      98       7           2.98\n2 Prediabetes/Diabet…     31.9         31      13      98       8           4.46\n# ℹ 9 more variables: median_ment_hlth &lt;dbl&gt;, min_ment_hlth &lt;dbl&gt;,\n#   max_ment_hlth &lt;dbl&gt;, IQR_ment_hlth &lt;dbl&gt;, mean_phys_hlth &lt;dbl&gt;,\n#   median_phys_hlth &lt;dbl&gt;, min_phys_hlth &lt;dbl&gt;, max_phys_hlth &lt;dbl&gt;,\n#   IQR_phys_hlth &lt;dbl&gt;\n\n\nAs expected, the mean BMI for individuals with diabetes/prediabetes is higher ,at 31, than those without diabetes at 27. Over 30 is considered to be obese, which obesity has been considered a major risk factor for heart disease. The average number of days of physical illness or injury in the past 30 days is also much higher for those with (pre)diabetes at 8 days verses those without at 3.6 days.\nThe following contingency table will give the number of individuals with/without diabetes based on their age group.\n\ntable(diabetes_data$age, diabetes_data$diabetes_binary)\n\n             \n              No diabetes Prediabetes/Diabetes\n  18-24              5622                   78\n  25-29              7458                  140\n  30-34             10809                  314\n  35-39             13197                  626\n  40-44             15106                 1051\n  45-49             18077                 1742\n  50-54             23226                 3088\n  55-59             26569                 4263\n  60-64             27511                 5733\n  65-69             25636                 6558\n  70-74             18392                 5141\n  75-79             12577                 3403\n  80 or older       14154                 3209\n\n\nThe highest number of individuals with diabetes/prediabetes occurs in the 65-69 age group, and the lowest is the youngest age group, 18-24.\nThe following contingency table will give us the number of individuals with/without diabetes by sex and by whether they’ve had heart disease or a heart attack.\n\ntable(diabetes_data$heart_diseaseor_attack, \n      diabetes_data$sex, \n      diabetes_data$diabetes_binary)\n\n, ,  = No diabetes\n\n                         \n                          Female   Male\n  No heart disease/attack 116716  85603\n  Heart disease/attack      6847   9168\n\n, ,  = Prediabetes/Diabetes\n\n                         \n                          Female   Male\n  No heart disease/attack  15053  12415\n  Heart disease/attack      3358   4520\n\n\nIt may be surprising to see that the number of individuals who have heart disease or who have had a heart attack is higher for those without diabetes (for both sexes) than those who do have diabetes/prediabetes.\nLet’s view some relationships visually.\nThe following barplot displays the number of individuals with/without diabetes according to smoker status and whether or not they’ve had heart disease or heart attack.\n\ng &lt;- ggplot(data = diabetes_data)\n\ng + geom_bar(aes(x = diabetes_binary, fill = smoker), position = \"dodge\") +\n  labs(x = \"Diabetes Status\", \n       y = \"Count\",\n       title = \"Number of Individuals per Diabetes Status\", \n       fill = \"Smoker Status\") +\n  easy_center_title() +\n  facet_wrap(~ heart_diseaseor_attack)\n\n\n\n\n\n\n\n\nFor those who have had a heart attack/disease, more individuals were smokers than non-smokers, but it did not seem to be an indicator of diabetes (there were more smokers and individuals with a heart attack/disease that did not have diabetes). For those who have not had a heart attack or heart disease, smoking status was almost equal (non-smoker vs smoker) for those with diabetes.\nThe following violin plot will take a look at BMI related to Sex and Diabetes Status.\n\nggplot(diabetes_data, aes(x = sex, y = bmi, fill = diabetes_binary)) +\n  geom_violin() +\n  labs(x = \"Sex\", \n       y = \"BMI\",\n       title = \"BMI by Sex and Diabetes Status\",\n       fill = \"Diabetes Status\") +\n  easy_center_title() +\n  scale_fill_manual(values = c(\"lightpink\",\n                               \"lightblue\"))\n\n\n\n\n\n\n\n\nThe BMI for both sexes is higher for those with diabetes/prediabetes.\nThe following density plot also gives the relationship between BMI and diabetes status.\n\ng + geom_density(aes(x = bmi, fill = diabetes_binary), alpha = 0.5) +\n  labs(x = \"BMI\", \n       y = \"Density\",\n       title = \"BMI by Diabetes Status\", \n       fill = \"Diabetes Status\") +\n  easy_center_title() \n\n\n\n\n\n\n\n\nJust as the violin plot above showed, those with prediabetes/diabetes tend to have a higher BMI.\nThe following bloxplot will also look at BMI and diabetes status, by those with/without heart disease or heart attack.\n\nggplot(diabetes_data, aes(x = diabetes_binary, y = bmi, fill = heart_diseaseor_attack)) +\n  geom_boxplot() +\n  labs(x = \"Diabetes Status\", \n       y = \"BMI\",\n       title = \"BMI by Sex and Diabetes Status\",\n       fill = \"Heart Disease/Attack Status\") +\n  easy_center_title() +\n  scale_fill_manual(values = c(\"lightpink\",\n                               \"lightblue\"))\n\n\n\n\n\n\n\n\nAs the other graphs have shown, the mean BMI is greater for those with diabetes/prediabetes. However, the mean BMI is approximately the same (in both diabetes status groups) between those with heart disease/attack and those without.\nThe following density plot show the BMI by diabetes status and physical activity level.\n\ng + geom_density(aes(x = bmi, fill = diabetes_binary), alpha = 0.5) +\n  labs(x = \"BMI\", \n       y = \"Density\",\n       title = \"BMI by Diabetes Status\",\n       subtitle = \"compared to physical activity level\",\n       fill = \"Diabetes Status\") +\n  easy_center_title() +\n  theme(plot.subtitle = element_text(hjust = 0.5)) +\n  facet_wrap(~ phys_activity)\n\n\n\n\n\n\n\n\nThe density plot shifts slightly right for those who have not had physical activity (outside of work) in the past 30 days, which means more of the individuals that have not had physical activity have higher BMI’s.\nThe following barplot shows general health based on diabetes status.\n\ng + geom_bar(aes(x = gen_hlth, fill = diabetes_binary), position = \"dodge\") +\n  labs(x = \"General Health Condition\", \n       y = \"Count\",\n       title = \"Diabetes Status based on General Health\", \n       fill = \"Diabetes Status\") +\n  easy_center_title()\n\n\n\n\n\n\n\n\nAs health condition worsens, the number of those without diabetes decreases. It is surprising to see that the greatest number of individuals with prediabetes/diabetes have “good” health.\nNow that we’ve seen some relationships in the data, let’s take a look at some models that we can use to predict diabetes status.\nClick here to continue to modeling"
  }
]